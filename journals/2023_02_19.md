- [[周报]]
	- ## 概述
		- 本周主要对在 "使用FedGen框架训练VP预测模型" 时遇到的问题进行原因分析, 思考了各种改进思路, 并实践了一些解决方案.
	- ## 本周遇到的核心问题
		- ### 使用FedGen以后, VP模型性能不增反减, 甚至不如纯本地训练
			- ==说得更本质一些, 是Generator_model的性能过差且对VP模型训练的干预过大.==
	- ## 原因分析 & 解决方案
		- ### VP模型结构的问题? 是不是LSTM不太行?
			- > **solution**: 尝试Transformer ==(已实现)==
		- ### FedGen框架的问题? 两种模型是"相互促进"还是"彼此拖累"?
			- FedGen框架中, VP模型的训练需要Generator_model, Generator_model的训练又需要VP模型; 因此, 如果两种模型的性能都比较高, 确实可以相互促进, 良性循环. 但是在训练最开始, VP_model和Generator_model的性能都很差, 这就很可能会彼此拖累, 恶性循环.
			- > **solution**: 对Generator模型进行预训练. 具体实现时, 是将训练划分为两个阶段: ==(已实现, 但没来得及测试效果, 因为程序运行比较慢)==
			  阶段一: VP_models的训练不借助Generator, 但Generator的训练借助VP_models
			  阶段二: VP_models和Generator彼此借助进行训练
		- ### Generator_model的问题?
			- Generator_model在训练时的loss设计有问题?
				- > **solution**: 去掉diversity_loss, 只保留teacher_loss ==(已尝试, 效果显著)==
			- Generator_model是不是没有能力完成 y-->z 这一比较复杂的 "倒推" 任务?
				- Generator_model的结构太简单 (fc_layer+bn_layer+ReLU+fc_layer)
					- > **solution**: 给Generator_model设计更复杂的模型结构 ==(未尝试)==
				- Generator_model的输入信息不足 (只以y为输入)
					- > **solution**: 将y和salmap作为Generator_model的输入 ==(未尝试)==
			- Generator_model得到的视点分布信息没有对具体视频内容的针对性
				- > **solution**: 将salmap作为对视点分布的近似 ==(未尝试)==
			- > **!!! Note !!!**
			  在上面提到的两个solutions中都提到了向Generator_model中引入salmap. 但是, 如果向FedGen框架中的Generator_model引入salmap, 则必须要进行在线训练, 这是完全另一条思路. 对于这一思路, 已与师兄进行过深入探讨, 最终的结论是不太看好该思路. 对该思路的详细整理见 : [[向FedGen的Generator中引入salmap]]
	- ## 使用Transformer模型, 并在local, FedAvg, FedGen框架下进行性能测试与比较 (目前这部分还没有测试全, 因为FedGen框架中的Generator在每次训练时都要给每个用户生成他此次train_batch的heatmap; 即需要对该用户当前train_batch中的每个label, 即视点, 做高斯模糊后叠加. 下周会继续整理这一部分, 同时增加生成heatmap的并行度, 提高程序运行效率)
		- local: 本地训练:   Average Global Loss = 0.0178
		  collapsed:: true
			- python main.py --train --dataset David-m10-h1 --algorithm local --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
		- FedAvg: 在服务器对各VP模型参数进行平均后下发
			- 全部下发 & 全部覆盖:  Average Global Loss = 0.0086
				- python main.py --train --dataset David-m10-h1 --algorithm FedAvg --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
			- 只下发最后两层 & 全部覆盖: Average Global Loss = 0.0097
			  collapsed:: true
				- python main.py --train --dataset David-m10-h1 --algorithm partialFedAvg --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
			- 全部下发 & 以0.5的权重进行覆盖: Average Global Loss = 0.0088.
			  collapsed:: true
				- python main.py --train --dataset David-m10-h1 --algorithm FedAvg --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
			- 只下发最后两层 & 以0.5的权重进行覆盖: Average Global Loss = 0.0113.
			  collapsed:: true
				- python main.py --train --dataset David-m10-h1 --algorithm partialFedAvg --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
		- FedGen
			- 不对Generator进行预训练:
				- 不下发: Average Global Loss = 0.0238.
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm localFedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 全部下发 & 全部覆盖: Average Global Loss = 0.0087 (第135个epoch就达到了0.0088, 0.0087是第300个epoch的结果; 以目前的程序效率, 跑300个epoch花了2.7h)
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm FedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 只下发最后两层 & 全部覆盖: Average Global Loss = 0.0119.
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm partialFedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 全部下发 & 以0.5的权重进行覆盖: Average Global Loss = 0.0091.
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm FedGen --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 只下发最后两层 & 以0.5的权重进行覆盖: Average Global Loss = 0.0157
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm partialFedGen --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
			- 对Generator进行预训练:
				- 不下发: Average Global Loss = 0.0089 (本来经过预训练阶段中的200个epoch, loss已达到0.0086了, 结果用Generator继续训练之后, loss不降反升, 升到0.0117, 然后又慢慢下降, 降到0.0089)
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm localFedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 全部下发 & 全部覆盖: Average Global Loss = 0.0086 (loss也是从0.0086上升到了0.0109, 然后再下降)
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm FedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 只下发最后两层 & 全部覆盖: Average Global Loss = 0.0087
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm partialFedGen --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 全部下发 & 以0.5的权重进行覆盖: Average Global Loss = 0.0086
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm FedGen --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
				- 只下发最后两层 & 以0.5的权重进行覆盖: Average Global Loss = 0.0087
				  collapsed:: true
					- python main.py --train --dataset David-m10-h1 --algorithm partialFedGen --beta 0.5 --model Transformer --embedding --device cuda --learning_rate 0.01 --batch_size 64 --gen_batch_size 256 --times 1 --num_glob_iters 300 --local_epochs 10 --num_users 10 --grain 256
	- ## 其它整理 (不太重要)
	  collapsed:: true
		- ### 对Generator所需的labels分布的处理
			- FedGen中的Generator_model在训练时需要每个用户的labels分布信息. 原代码针对分类问题进行讨论, labels分布的获取较为简单直接. 而在我们要解决的视点预测问题中, labels分布实际上就是视点的分布. 所以client可以将自己每次train_batch中的y数据, 也就是labels数据, 直接上传到server端. 你可能会问, 你不是要隐私保护吗? 这里怎么能直接上传视点位置呢? 实际上, user_model每次的train_batch, 都是从本地训练集中随机sample的, 所以我们上传的labels实际上是没有时序信息的视点, 这是完全可以的, 并不会破坏用户隐私.
			- server端会将每个用户上传的labels(视点位置)做高斯模糊, 生成heatmap, 作为此轮训练中该用户的视点分布.
		- ### 关于性能测试
			- 可以画loss随epoch的变化图
		- ### 核心点在哪?
			- 总感觉, FedGen是已有的, Transformer也是已有的, 目前的工作只是针对具体的VP预测任务, 做了一些适配, 并没有创新点